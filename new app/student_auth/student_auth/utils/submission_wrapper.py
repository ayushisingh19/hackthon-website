#!/usr/bin/env python3
"""
Submission Wrapper for Competitive Programming Platform

This module provides a comprehensive system for wrapping, executing, and evaluating
user code submissions in a competitive programming environment. It handles:

1. **Code Injection**: Seamlessly integrates user solutions into complete programs
2. **Multi-language Support**: Supports Python and C++ with extensible architecture
3. **Performance Testing**: Executes submissions against comprehensive test suites
4. **Baseline Correc            # === CODE INJECTION PHASE ===
            # Wrap user code with problem-specific template
            if language == "python":
                wrapped_code = self.wrap_python_code(user_code, template)
            elif language == "cpp":
                wrapped_code = self.wrap_cpp_code(user_code, template)
            
            # === EXECUTION CONFIGURATION ===
            # Extract execution limits and baseline from test suite metadata
            metadata = test_data["metadata"]
            tle_limit = metadata.get("tle_limit", 1.0)       # Time limit for submissions
            mle_limit = metadata.get("mle_limit", 64000)     # Memory limit for submissions
            baseline_time = metadata.get("baseline_time", 0.0)     # Admin baseline time
            baseline_memory = metadata.get("baseline_memory", 0)   # Admin baseline memory
            
            # === RESULT INITIALIZATION ===
            # Initialize results structure compatible with admin test case format
            results = {
                "metadata": {
                    "problem": self.problem_name,
                    "language": language,
                    "language_id": Config.LANGUAGE_CONFIGS[language]["id"],line subtraction for accurate algorithm timing
5. **Result Generation**: Produces detailed JSON reports compatible with scoring systems

Key Features:
- Regex-based code injection that preserves solution templates
- Judge0 integration for secure code execution
- Comprehensive error handling and timeout management
- Performance measurement with baseline subtraction
- Stop-on-error optimization for fast feedback
- Batch processing support for multiple submissions

Architecture:
- SubmissionWrapper: Main wrapper class coordinating all operations
- Language-specific code injection methods (Python, C++)
- Judge0 executor with performance measurement
- Comprehensive result formatting and JSON output

Usage:
    wrapper = SubmissionWrapper("merge_k_lists")
    result = wrapper.evaluate_submission("user_solution.py", "python")
    
Integration:
    Compatible with submission_scorer.py for complete evaluation pipeline
    Designed for web platform integration with 400+ participant support

Author: Competitive Programming Platform Team
Date: September 2025
License: Educational Use
"""

import os
import sys
import json
import requests
import time
import re
from typing import Dict, List, Tuple, Optional


class Config:
    """
    Configuration class for submission wrapper settings.
    
    Centralizes all configuration values to enable easy modification
    and environment-specific adjustments.
    """
    # Judge0 API configuration
    JUDGE0_URL = "http://localhost:3000"
    
    # Directory structure
    PROBLEMS_DIR = "problems"
    
    # Language support mapping
    LANGUAGE_CONFIGS = {
        "python": {"id": 71, "file_ext": "py"},    # Python 3.8.1
        "cpp": {"id": 54, "file_ext": "cpp"}       # C++17 (GCC 9.2.0)
    }


class SubmissionWrapper:
    """
    Main wrapper class for processing and evaluating user code submissions.
    
    This class coordinates the entire submission evaluation process:
    1. Loading test cases and solution templates
    2. Injecting user code into complete programs
    3. Executing submissions via Judge0 API
    4. Collecting performance metrics with baseline correction
    5. Generating comprehensive evaluation results
    
    The wrapper is designed to handle incomplete user submissions by
    intelligently merging them with problem-specific templates while
    preserving the algorithmic core provided by the user.
    """
    
    def __init__(self, problem_name: str):
        """
        Initialize the submission wrapper for a specific problem.
        
        Args:
            problem_name (str): Name of the problem directory in PROBLEMS_DIR
                              Must contain solution templates and test cases
        """
        self.problem_name = problem_name
        self.problem_path = os.path.join(Config.PROBLEMS_DIR, problem_name)
        
    def load_test_cases(self, language: str) -> Dict:
        """
        Load language-specific test cases from generated JSON files.
        
        Loads the complete test suite generated by the orchestrator,
        including both visible and hidden test cases with performance
        benchmarks from admin solutions.
        
        Args:
            language (str): Target language ('python', 'cpp', etc.)
            
        Returns:
            Dict: Complete test suite data structure containing:
                - metadata: Test suite information and limits
                - test_cases: List of test case objects with:
                    - stdin: Input data
                    - expected_output: Correct output
                    - admin_time: Admin solution baseline time
                    - admin_memory: Admin solution baseline memory
                    - is_visible: Visibility flag
                    
        Raises:
            FileNotFoundError: If test cases file doesn't exist
        """
        test_file = f"{self.problem_name}_{language}_testcases.json"
        if not os.path.exists(test_file):
            raise FileNotFoundError(f"Test cases file not found: {test_file}")
        
        with open(test_file, 'r') as f:
            return json.load(f)
    
    def load_template(self, language: str) -> str:
        """
        Load the solution template for code injection.
        
        The template contains the complete program structure including:
        - Import statements and includes
        - I/O handling code
        - Data structure definitions
        - Main function and boilerplate
        - Placeholder for user algorithm implementation
        
        The user's code will be injected into this template to create
        a complete, executable program.
        
        Args:
            language (str): Target language for template loading
            
        Returns:
            str: Complete template source code with injection points
            
        Raises:
            FileNotFoundError: If template file doesn't exist
        """
        lang_config = Config.LANGUAGE_CONFIGS[language]
        template_file = os.path.join(self.problem_path, f"solution.{lang_config['file_ext']}")
        
        if not os.path.exists(template_file):
            raise FileNotFoundError(f"Template file not found: {template_file}")
        
        with open(template_file, 'r') as f:
            return f.read()
    
    def wrap_python_code(self, user_code: str, template: str) -> str:
        """
        Intelligently inject user Python code into the solution template.
        
        This method implements sophisticated code injection that:
        1. Extracts the user's core algorithm implementation
        2. Preserves the template's I/O and structure handling
        3. Maintains proper indentation and syntax
        4. Handles various user coding styles and patterns
        
        The injection process uses regex patterns to identify and replace
        the placeholder implementation in the template with the user's
        algorithm while preserving all necessary boilerplate.
        
        Code Injection Strategy:
        - Identifies class methods, functions, or main algorithm blocks
        - Preserves imports and helper functions from template
        - Maintains proper Python indentation and structure
        - Handles edge cases like incomplete submissions
        
        Args:
            user_code (str): User's algorithm implementation (may be incomplete)
            template (str): Complete solution template with I/O handling
            
        Returns:
            str: Complete executable Python program with user's algorithm
            
        Example:
            User provides: "def mergeKLists(self, lists): return merge_logic"
            Template has: Full ListNode class + I/O + placeholder
            Result: Complete program with user's mergeKLists injected
        """
        user_code = user_code.strip()
        
        # === PYTHON CODE INJECTION STRATEGY ===
        # 1. Try to find complete Solution class in user code
        # 2. If not found, extract just the core method and wrap it
        # 3. Inject the user's algorithm into the template structure
        
        # Pattern to match complete Solution class with flexible whitespace and inheritance
        solution_pattern = r'class\s+Solution\s*\([^)]*\)\s*:.*?(?=\n\S|\Z)'
        
        user_solution_match = re.search(solution_pattern, user_code, re.DOTALL)
        if not user_solution_match:
            # === FALLBACK: EXTRACT METHOD ONLY ===
            # If user didn't provide complete class, try to find just the core method
            method_pattern = r'def\s+mergeKLists\s*\([^)]*\)\s*:.*?(?=\ndef\s+|\nclass\s+|\Z)'
            method_match = re.search(method_pattern, user_code, re.DOTALL)
            
            if method_match:
                # Wrap user's method in proper Solution class structure
                user_method = method_match.group(0)
                # Properly indent the method for class membership
                indented_method = '\n'.join('    ' + line if line.strip() else line 
                                          for line in user_method.split('\n'))
                user_solution = f"class Solution(object):\n{indented_method}"
            else:
                raise ValueError("Could not find mergeKLists method in user code")
        else:
            # Use the complete Solution class provided by user
            user_solution = user_solution_match.group(0)
        
        # === TEMPLATE INJECTION ===
        # Replace the placeholder Solution class in template with user's implementation
        # Pattern matches from class definition to the I/O handling section marker
        template_pattern = r'class\s+Solution\s*\([^)]*\)\s*:.*?(?=\n# --- Input/Output Handling ---)'
        wrapped_code = re.sub(template_pattern, user_solution, template, flags=re.DOTALL)
        
        return wrapped_code
    
    def wrap_cpp_code(self, user_code: str, template: str) -> str:
        """
        Intelligently inject user C++ code into the solution template.
        
        This method handles C++ code injection with similar sophistication to Python:
        1. Attempts to extract complete Solution class from user code
        2. Falls back to method extraction if class not found
        3. Maintains proper C++ syntax and structure
        4. Preserves template's includes and I/O handling
        
        C++ Injection Strategy:
        - Identifies complete class definitions with public methods
        - Handles method-only submissions by wrapping in class
        - Preserves proper C++ syntax including semicolons and braces
        - Maintains template's data structure definitions and main function
        
        Args:
            user_code (str): User's C++ algorithm implementation
            template (str): Complete C++ template with includes and I/O
            
        Returns:
            str: Complete executable C++ program with user's algorithm
            
        Example:
            User provides: "ListNode* mergeKLists(vector<ListNode*>& lists) {...}"
            Template has: Full ListNode struct + includes + main + placeholder
            Result: Complete C++ program with user's method injected
        """
        user_code = user_code.strip()
        
        # === C++ CODE INJECTION STRATEGY ===
        # 1. Try to find complete Solution class with proper C++ syntax
        # 2. If not found, extract core method and wrap in class structure
        # 3. Inject into template while preserving includes and main function
        
        # Pattern to match complete Solution class with public methods
        solution_pattern = r'class\s+Solution\s*\{[^}]*public\s*:.*?\};'
        user_solution_match = re.search(solution_pattern, user_code, re.DOTALL)
        
        if not user_solution_match:
            # === FALLBACK: EXTRACT METHOD ONLY ===
            # Try to find just the core mergeKLists method
            method_pattern = r'ListNode\*\s+mergeKLists\s*\([^)]*\)\s*\{.*?\}'
            method_match = re.search(method_pattern, user_code, re.DOTALL)
            
            if method_match:
                # Wrap user's method in proper Solution class structure
                user_method = method_match.group(0)
                user_solution = f"class Solution {{\npublic:\n    {user_method}\n}};"
            else:
                raise ValueError("Could not find mergeKLists method in user code")
        else:
            # Use the complete Solution class provided by user
            user_solution = user_solution_match.group(0)
        
        # === TEMPLATE INJECTION ===
        # Replace the placeholder Solution class in template with user's implementation
        template_pattern = r'class\s+Solution\s*\{.*?\};'
        wrapped_code = re.sub(template_pattern, user_solution, template, flags=re.DOTALL)
        
        return wrapped_code
    
    def execute_code(self, source_code: str, test_input: str, language_id: int, tle_limit: float, mle_limit: int) -> Dict:
        """
        Execute wrapped code via Judge0 API with comprehensive error handling.
        
        This method handles the critical task of executing user submissions in a
        secure, controlled environment while collecting detailed performance metrics
        and handling various failure modes gracefully.
        
        Key Features:
        - Secure code execution via Judge0 API
        - Comprehensive timeout and memory limit enforcement
        - Detailed error categorization and reporting
        - Performance metric collection for evaluation
        - Proper status code handling for all execution outcomes
        
        Args:
            source_code (str): Complete wrapped program ready for execution
            test_input (str): Input data for the test case
            language_id (int): Judge0 language identifier
            tle_limit (float): Time limit in seconds
            mle_limit (int): Memory limit in KB
            
        Returns:
            Dict: Execution result containing:
                - status: Execution status ('AC', 'WA', 'TLE', 'MLE', 'CE', 'RE')
                - user_time: Execution time in seconds (0 if error)
                - user_memory: Memory usage in KB (0 if error)
                - user_output: Program output (empty string if error)
                - error_message: Detailed error description (if applicable)
                
        Status Codes:
        - AC: Accepted (successful execution)
        - WA: Wrong Answer (incorrect output)
        - TLE: Time Limit Exceeded
        - MLE: Memory Limit Exceeded  
        - CE: Compilation Error
        - RE: Runtime Error
        """
        try:
            # === JUDGE0 API SUBMISSION ===
            response = requests.post(
                f"{Config.JUDGE0_URL}/submissions?base64_encoded=false&wait=true",
                json={
                    "source_code": source_code,
                    "language_id": language_id,
                    "stdin": test_input
                }
            )
            response.raise_for_status()
            result = response.json()
            
            # === RESULT PARSING ===
            # Extract execution results and status information from Judge0 response
            status_id = result.get("status", {}).get("id")
            status_description = result.get("status", {}).get("description", "Unknown")
            
            raw_time = float(result.get("time") or 0)
            raw_memory = int(result.get("memory") or 0)
            stdout = (result.get("stdout") or "").strip()
            stderr = (result.get("stderr") or "").strip()
            
            # === ERROR CONDITION HANDLING ===
            # Process various failure modes with appropriate status classification
            
            if status_id == 5:  # Time Limit Exceeded
                return {
                    "status": "TLE",
                    "user_time": tle_limit,      # Use limit as reported time
                    "user_memory": raw_memory,
                    "stdout": stdout,
                    "stderr": stderr,
                    "raw_time": raw_time,
                    "raw_memory": raw_memory
                }
                
            elif status_id == 6:  # Compilation Error
                return {
                    "status": "CE",
                    "user_time": 0,              # No execution time for compilation errors
                    "user_memory": 0,            # No memory usage for compilation errors
                    "stdout": stdout,
                    "stderr": stderr,
                    "raw_time": raw_time,
                    "raw_memory": raw_memory
                }
                
            elif status_id != 3:  # Not Accepted (Runtime Error, etc.)
                return {
                    "status": "RE",
                    "user_time": raw_time,
                    "user_memory": raw_memory,
                    "stdout": stdout,
                    "stderr": stderr,
                    "raw_time": raw_time,
                    "raw_memory": raw_memory,
                    "status_description": status_description
                }
            
            # === RESOURCE LIMIT CHECKS ===
            # Additional checks for resource limits that Judge0 might miss
            
            # Memory Limit Exceeded check
            if raw_memory > mle_limit:
                return {
                    "status": "MLE",
                    "user_time": raw_time,
                    "user_memory": raw_memory,
                    "stdout": stdout,
                    "stderr": stderr,
                    "raw_time": raw_time,
                    "raw_memory": raw_memory
                }
            
            # Time Limit Exceeded check (backup for Judge0)
            if raw_time > tle_limit:
                return {
                    "status": "TLE",
                    "user_time": tle_limit,
                    "user_memory": raw_memory,
                    "stdout": stdout,
                    "stderr": stderr,
                    "raw_time": raw_time,
                    "raw_memory": raw_memory
                }
            
            # === SUCCESS CASE ===
            # Successful execution with valid output
            return {
                "status": "AC",
                "user_time": raw_time,
                "user_memory": raw_memory,
                "stdout": stdout,
                "stderr": stderr,
                "raw_time": raw_time,
                "raw_memory": raw_memory
            }
            
        except requests.exceptions.RequestException as e:
            return {
                "status": "ERROR",
                "error": f"Judge0 request failed: {str(e)}",
                "user_time": 0,
                "user_memory": 0,
                "stderr": f"Request error: {str(e)}"
            }
        except Exception as e:
            return {
                "status": "ERROR",
                "error": f"Execution error: {str(e)}",
                "user_time": 0,
                "user_memory": 0,
                "stderr": f"Execution error: {str(e)}"
            }
    
    def evaluate_submission(self, user_code: str, language: str, 
                          max_cases: Optional[int] = None, 
                          stop_on_first_error: bool = True) -> Dict:
        """
        Comprehensive evaluation of user submission against complete test suite.
        
        This is the main entry point for submission evaluation. It coordinates
        the entire evaluation process from code injection through result compilation:
        
        1. **Input Validation**: Verifies language support and file availability
        2. **Code Injection**: Wraps user code with problem-specific template
        3. **Test Execution**: Runs submission against all test cases
        4. **Performance Analysis**: Applies baseline correction for accurate timing
        5. **Result Compilation**: Generates comprehensive JSON evaluation report
        
        Key Features:
        - Language-agnostic evaluation (Python, C++)
        - Stop-on-error optimization for fast feedback
        - Baseline-corrected performance measurement
        - Comprehensive error handling and categorization
        - JSON output compatible with scoring systems
        
        Args:
            user_code (str): User's algorithm implementation (may be incomplete)
            language (str): Target language ('python', 'cpp')
            max_cases (int, optional): Maximum number of test cases to run
            stop_on_first_error (bool): Whether to stop on first non-AC result
            
        Returns:
            Dict: Comprehensive evaluation result containing:
                - metadata: Problem and language information
                - summary: Aggregate statistics (AC, WA, TLE, MLE, CE, RE counts)
                - test_cases: Detailed results for each test case
                - user_code: Original user submission (for reference)
                - wrapped_code: Complete program after injection
                - tle_limit: Time limit used for evaluation
                - mle_limit: Memory limit used for evaluation
                
        Example Result Structure:
            {
                "metadata": {"problem": "merge_k_lists", "language": "python"},
                "summary": {"ac": 8, "wa": 2, "tle": 0, "total": 10},
                "test_cases": [
                    {
                        "test_case_no": 1,
                        "status": "AC",
                        "user_time": 0.012,
                        "user_memory": 2400,
                        "admin_time": 0.008,
                        "expected_output": "[1,1,2,3,4,4,5,6]",
                        "is_visible": true
                    },
                    ...
                ],
                "user_code": "def mergeKLists(self, lists): ...",
                "wrapped_code": "# Complete program...",
                "tle_limit": 3.0,
                "mle_limit": 128000
            }
        """
        # === INPUT VALIDATION ===
        if language not in Config.LANGUAGE_CONFIGS:
            return {"error": f"Unsupported language: {language}"}
        
        try:
            # === SETUP PHASE ===
            # Load test cases and template
            test_data = self.load_test_cases(language)
            template = self.load_template(language)
            
            # === CODE INJECTION PHASE ===
            # Wrap user code with problem-specific template
            if language == "python":
                wrapped_code = self.wrap_python_code(user_code, template)
            elif language == "cpp":
                wrapped_code = self.wrap_cpp_code(user_code, template)
            
            # Get limits from metadata
            metadata = test_data["metadata"]
            tle_limit = metadata.get("tle_limit", 1.0)
            mle_limit = metadata.get("mle_limit", 64000)
            baseline_time = metadata.get("baseline_time", 0.0)
            baseline_memory = metadata.get("baseline_memory", 0)
            
            # === RESULT INITIALIZATION ===
            # Initialize results structure compatible with admin test case format
            results = {
                "metadata": {
                    "problem_name": self.problem_name,
                    "language": language,
                    "language_id": Config.LANGUAGE_CONFIGS[language]["id"],
                    "total_test_cases": metadata["total_test_cases"],
                    "visible_cases": metadata["visible_cases"],
                    "hidden_cases": metadata["hidden_cases"],
                    "baseline_time": baseline_time,
                    "baseline_memory": baseline_memory,
                    "tle_limit": tle_limit,
                    "mle_limit": mle_limit,
                    "submission_timestamp": time.time()
                },
                "test_cases": [],
                "summary": {
                    # Detailed execution statistics for performance analysis
                    "passed": 0,                    # Correct outputs (AC)
                    "failed": 0,                    # Wrong answers (WA)
                    "tle": 0,                       # Time limit exceeded
                    "mle": 0,                       # Memory limit exceeded
                    "ce": 0,                        # Compilation errors
                    "re": 0,                        # Runtime errors
                    "total_effective_time": 0,      # Sum of baseline-corrected times
                    "max_effective_time": 0,        # Maximum baseline-corrected time
                    "total_effective_memory": 0,    # Sum of baseline-corrected memory
                    "max_effective_memory": 0       # Maximum baseline-corrected memory
                }
            }
            
            # === TEST EXECUTION PHASE ===
            # Execute submission against test cases with performance measurement
            test_cases = test_data["test_cases"]
            if max_cases:
                test_cases = test_cases[:max_cases]  # Limit for testing/debugging
            
            for i, test_case in enumerate(test_cases):
                # Extract test case data
                test_input = test_case.get("stdin", "")
                expected_output = test_case.get("expected_output", "").strip()
                is_visible = test_case.get("is_visible", False)
                
                print(f"Running test case {i + 1}/{len(test_cases)}...", end=" ")
                
                # === INDIVIDUAL TEST EXECUTION ===
                # Execute wrapped code against current test case
                execution_result = self.execute_code(
                    wrapped_code, 
                    test_input, 
                    Config.LANGUAGE_CONFIGS[language]["id"],
                    tle_limit,
                    mle_limit
                )
                
                # === CORRECTNESS EVALUATION ===
                # Compare actual output with expected output
                actual_output = execution_result.get("stdout", "").strip()
                is_correct = (execution_result["status"] == "AC" and actual_output == expected_output)
                
                # === PERFORMANCE MEASUREMENT ===
                # Apply baseline subtraction for accurate algorithm performance
                effective_time = max(0.0, execution_result["user_time"] - baseline_time)
                effective_memory = max(baseline_memory * 0.05, execution_result["user_memory"] - baseline_memory)
                
                # === TEST RESULT COMPILATION ===
                # Create detailed test case result with all metrics
                test_result = {
                    "test_case_no": i + 1,
                    "status": execution_result["status"],
                    "is_correct": is_correct,
                    "user_time": effective_time,           # Baseline-corrected algorithm time
                    "user_memory": effective_memory,       # Baseline-corrected algorithm memory
                    "raw_time": execution_result["user_time"],    # Raw execution time from Judge0
                    "raw_memory": execution_result["user_memory"], # Raw memory usage from Judge0
                    "baseline_time": baseline_time,        # Applied baseline correction
                    "baseline_memory": baseline_memory,    # Applied baseline correction
                    "is_visible": is_visible               # Visibility flag for contestants
                }
                
                # === VISIBILITY HANDLING ===
                # Add detailed output information only for visible test cases
                if is_visible:
                    test_result["expected_output"] = expected_output
                    test_result["actual_output"] = actual_output
                    if execution_result.get("stderr"):
                        test_result["stderr"] = execution_result["stderr"]
                
                results["test_cases"].append(test_result)
                
                # === STATUS FEEDBACK ===
                # Provide immediate feedback for each test case
                if execution_result["status"] == "AC" and is_correct:
                    print("✅ AC")
                elif execution_result["status"] == "TLE":
                    print("⏰ TLE")
                elif execution_result["status"] == "MLE":
                    print("💾 MLE")
                elif execution_result["status"] == "CE":
                    print("🔨 CE")
                elif execution_result["status"] == "RE":
                    print("💥 RE")
                else:
                    print("❌ WA")
                
                # === STATISTICS UPDATE ===
                # Update aggregate statistics for summary
                summary = results["summary"]
                if execution_result["status"] == "AC" and is_correct:
                    summary["passed"] += 1
                elif execution_result["status"] == "TLE":
                    summary["tle"] += 1
                elif execution_result["status"] == "MLE":
                    summary["mle"] += 1
                elif execution_result["status"] == "CE":
                    summary["ce"] += 1
                elif execution_result["status"] == "RE":
                    summary["re"] += 1
                else:
                    summary["failed"] += 1
                
                summary["total_effective_time"] += effective_time
                summary["max_effective_time"] = max(summary["max_effective_time"], effective_time)
                summary["total_effective_memory"] += effective_memory
                summary["max_effective_memory"] = max(summary["max_effective_memory"], effective_memory)
                
                # Stop on first error if requested
                if stop_on_first_error and (execution_result["status"] in ["CE", "TLE", "MLE", "RE"] or 
                                           (execution_result["status"] == "AC" and not is_correct)):
                    print(f"\n⏹️  Stopping evaluation - {execution_result['status']} detected (Score = 0 per formula.txt)")
                    break
            
            return results
            
        except Exception as e:
            return {"error": f"Evaluation failed: {str(e)}"}

def main():
    """CLI interface for testing submissions."""
    if len(sys.argv) < 3:
        print("Usage: python submission_wrapper.py <problem_name> <language_or_folder> [user_code_file] [options]")
        print("Options:")
        print("  Single file: python submission_wrapper.py merge_k_lists python user_solution.py")
        print("  All files:   python submission_wrapper.py merge_k_lists submissions/merge_k_lists/")
        print("  --max-cases N         Run only first N test cases")
        print("  --continue-on-error   Continue testing all cases even after failures (default: stop on first error)")
        print("Note: By default, evaluation stops on first error since scoring formula requires 100% correctness.")
        sys.exit(1)
    
    problem_name = sys.argv[1]
    second_arg = sys.argv[2]
    
    # Check if second argument is a folder path
    if os.path.isdir(second_arg):
        # Folder mode - run all files in the folder
        folder_path = second_arg
        
        # Parse options for folder mode
        max_cases = None
        stop_on_error = True  # Default to stop on error
        
        for i, arg in enumerate(sys.argv[3:], 3):
            if arg == "--max-cases" and i + 1 < len(sys.argv):
                max_cases = int(sys.argv[i + 1])
            elif arg == "--continue-on-error":
                stop_on_error = False
        
        # Find all solution files in the folder
        solution_files = []
        for file in os.listdir(folder_path):
            if file.endswith('.py') or file.endswith('.cpp'):
                file_path = os.path.join(folder_path, file)
                if file.endswith('.py'):
                    language = 'python'
                else:
                    language = 'cpp'
                solution_files.append((file_path, language, file))
        
        if not solution_files:
            print(f"No solution files (.py or .cpp) found in {folder_path}")
            sys.exit(1)
        
        print(f"🎯 Found {len(solution_files)} solution files in {folder_path}")
        print("=" * 60)
        
        # Run each file
        all_results = []
        for i, (file_path, language, filename) in enumerate(solution_files, 1):
            print(f"\n📁 [{i}/{len(solution_files)}] Testing: {filename} ({language})")
            print("-" * 40)
            
            with open(file_path, 'r') as f:
                user_code = f.read()
            
            wrapper = SubmissionWrapper(problem_name)
            results = wrapper.evaluate_submission(user_code, language, max_cases, stop_on_error)
            
            if "error" in results:
                print(f"❌ Error in {filename}: {results['error']}")
                continue
            
            # Save individual result file for each submission
            individual_output_file = f"user_submission_{problem_name}_{language}_{filename.replace('.', '_')}_{int(time.time())}.json"
            with open(individual_output_file, 'w') as f:
                json.dump(results, f, indent=2)
            
            # Add filename to results for summary table
            results['filename'] = filename
            results['file_path'] = file_path
            results['individual_result_file'] = individual_output_file
            all_results.append(results)
            
            # Show quick summary for this file
            summary = results["summary"]
            metadata = results["metadata"]
            total_tested = len(results["test_cases"])
            
            print(f"📊 {filename} Results:")
            print(f"   ✅ Passed: {summary['passed']}/{total_tested}")
            if summary['passed'] == total_tested:
                print(f"   🎉 All tests passed! Max time: {summary['max_effective_time']:.6f}s")
            else:
                print(f"   ❌ Failed: {summary['failed']}, TLE: {summary['tle']}, MLE: {summary['mle']}, CE: {summary['ce']}")
            print(f"   📄 Individual results: {individual_output_file}")
        
        # Generate summary comparison report (without detailed test case data)
        if all_results:
            print(f"\n🏆 FOLDER SUMMARY COMPARISON")
            print("=" * 60)
            
            # Sort by performance (passed tests, then by time)
            all_results.sort(key=lambda x: (-x['summary']['passed'], x['summary']['max_effective_time']))
            
            print(f"{'Rank':<4} {'File':<30} {'Passed':<8} {'Max Time':<12} {'Status'}")
            print("-" * 65)
            
            for i, result in enumerate(all_results, 1):
                summary = result['summary']
                total_cases = len(result['test_cases'])
                max_time = summary['max_effective_time']
                
                if summary['passed'] == total_cases:
                    status = "🎉 PERFECT"
                elif summary['ce'] > 0:
                    status = "💥 COMPILE ERROR"
                elif summary['tle'] > 0:
                    status = "⏰ TOO SLOW"
                else:
                    status = "❌ WRONG ANSWER"
                
                print(f"{i:<4} {result['filename']:<30} {summary['passed']}/{total_cases:<8} {max_time:<12.6f} {status}")
            
            # Save lightweight summary (just metadata, no test case details)
            summary_data = {
                "folder_path": folder_path,
                "problem_name": problem_name,
                "total_files_tested": len(all_results),
                "test_timestamp": time.time(),
                "summary_results": []
            }
            
            for result in all_results:
                summary_data["summary_results"].append({
                    "filename": result['filename'],
                    "language": result['metadata']['language'],
                    "summary": result['summary'],
                    "metadata": result['metadata'],
                    "individual_result_file": result['individual_result_file']
                })
            
            summary_file = f"folder_summary_{problem_name}_{int(time.time())}.json"
            with open(summary_file, 'w') as f:
                json.dump(summary_data, f, indent=2)
            print(f"\n� Folder summary saved to: {summary_file}")
            print(f"📄 Individual detailed results saved as separate files for each submission")
        
    else:
        # Single file mode (original behavior)
        if len(sys.argv) < 4:
            print("Error: For single file mode, you need: python submission_wrapper.py <problem_name> <language> <user_code_file>")
            sys.exit(1)
        
        language = second_arg
        user_code_file = sys.argv[3]
        
        # Parse options for single file mode
        max_cases = None
        stop_on_error = True  # Default to stop on error
        
        for i, arg in enumerate(sys.argv[4:], 4):
            if arg == "--max-cases" and i + 1 < len(sys.argv):
                max_cases = int(sys.argv[i + 1])
            elif arg == "--continue-on-error":
                stop_on_error = False
        
        if not os.path.exists(user_code_file):
            print(f"Error: User code file not found: {user_code_file}")
            sys.exit(1)
        
        with open(user_code_file, 'r') as f:
            user_code = f.read()
        
        wrapper = SubmissionWrapper(problem_name)
        results = wrapper.evaluate_submission(user_code, language, max_cases, stop_on_error)
        
        if "error" in results:
            print(f"Error: {results['error']}")
            sys.exit(1)
        
        # Print results (existing single file display code)
        print(f"\n🎯 Submission Results for {problem_name} ({language})")
        print("=" * 60)
        
        summary = results["summary"]
        metadata = results["metadata"]
        total_tested = len(results["test_cases"])
        print(f"📊 Summary ({total_tested} test cases):")
        print(f"   ✅ Passed: {summary['passed']}/{total_tested}")
        print(f"   ❌ Failed: {summary['failed']}")
        print(f"   ⏰ TLE: {summary['tle']}")
        print(f"   💾 MLE: {summary['mle']}")
        print(f"   🔧 CE: {summary['ce']}")
        print(f"   💥 RE: {summary['re']}")
        print(f"   🕒 Max Time: {summary['max_effective_time']:.6f}s")
        print(f"   🧠 Max Memory: {summary['max_effective_memory']:.0f}KB")
        print(f"   ⚡ TLE Limit: {metadata['tle_limit']}s")
        print(f"   💽 MLE Limit: {metadata['mle_limit']}KB")
        
        # Show visible test case details  
        visible_cases = [result for result in results["test_cases"] if result.get("expected_output")]
        if visible_cases:
            print(f"\n📋 Visible Test Cases:")
            for result in visible_cases:
                status_emoji = "✅" if result["is_correct"] and result["status"] == "AC" else "❌"
                print(f"   {status_emoji} Case {result['test_case_no']}: {result['status']} "
                      f"({result['user_time']:.6f}s, {result['user_memory']:.0f}KB)")
                if not result["is_correct"] or result["status"] != "AC":
                    print(f"      Expected: {result.get('expected_output', 'N/A')}")
                    print(f"      Got:      {result.get('actual_output', 'N/A')}")
                    if result.get("stderr"):
                        print(f"      Error:    {result['stderr']}")
        
        # Performance verdict
        if summary['passed'] == total_tested:
            print(f"\n🎉 All {total_tested} test cases passed!")
        elif summary['ce'] > 0:
            print(f"\n💥 Compilation Error - Please fix your code syntax")
        elif summary['tle'] > 0:
            print(f"\n⏰ Time Limit Exceeded on {summary['tle']} cases - Algorithm may be too slow")
        elif summary['mle'] > 0:
            print(f"\n💾 Memory Limit Exceeded on {summary['mle']} cases - Optimize memory usage")
        else:
            print(f"\n❌ {summary['failed']} test cases failed - Check your logic")
        
        # Save detailed results
        output_file = f"user_submission_{problem_name}_{language}_{int(time.time())}.json"
        with open(output_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\n📄 Detailed results saved to: {output_file}")

if __name__ == "__main__":
    main()
